<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Robot Car</title>
<!-- 2017-01-20 Fri 19:40 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="David A. Ventimiglia" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<style>@import 'https://fonts.googleapis.com/css?family=Quattrocento';</style>
<link rel="stylesheet" type="text/css" href="base.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Robot Car</h1>

<div class="figure">
<p><img src="robotcar.jpg" alt="robotcar.jpg" width="100%" />
</p>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
In this document we report the results of a project for the <a href="https://www.udacity.com/">Udacity</a>
<a href="https://www.udacity.com/drive?utm_source=google&utm_medium=cpc&utm_campaign=brand&gclid=COuc4pLi0dECFUtNfgodvSAPcg">Self-Driving Car Engineer</a> Nanodegree program's first semester.  The
objective of the project is to build and train a model capable of
driving a simulated car.
</p>

<p>
The method is quite simple.  We somehow acquire training
data&#x2014;perhaps by recording it ourselves&#x2014;in a computerized driving
simulator.  The training data comprise images of the road as seen
from cameras mounted on the simulated car, along with corresponding
control inputs (in this case, just the steering angle).  The
training data are used to train a <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network</a> model
so that it recognizes road/car configurations and generates the
appropriate steering angle.  The model is then used to generate
inputs (steering angles) in real-time for the simulation, unpiloted
by a human driver.
</p>

<p>
Here is a brief enumeration of the elements of my project
laboratory.
</p>

<ul class="org-ul">
<li><a href="https://keras.io/">Keras</a> - Deep Learning toolkit for Python
</li>
<li><a href="https://www.tensorflow.org/">TensorFlow</a> - High-Performance numerical computation library for
Python and backend for Keras.
</li>
<li>Unix <a href="https://en.wikipedia.org/wiki/GNU_Core_Utilities">command-line tools</a> - handy for data pre-processing
</li>
<li><a href="https://www.gnu.org/software/emacs/">Emacs</a> - indispensable coding and writing environment
</li>
<li><a href="http://orgmode.org/">Org mode</a> - indispensable writing and publishing environment for
Emacs
</li>
<li><a href="http://shop.lenovo.com/us/en/laptops/ideapad/u-series/u310/">Lenovo IdeaPad U310</a> - somewhat ancient laptop
</li>
<li><a href="https://www.milibrary.org/">The Mechanic's Institute Library</a> - a calm oasis in downtown San
Francisco (shown below)
</li>
</ul>


<div class="figure">
<p><img src="rig.jpg" alt="rig.jpg" height="50%" width="50%" />
</p>
<p><span class="figure-number">Figure 2:</span> Project Lab</p>
</div>

<p>
While tools like Keras and TensorFlow (really, TensorFlow) are
tailor-made for modern high-performance parallel numerical
computation using <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>, environments that are easily-obtained with
cloud-computing environments like <a href="https://aws.amazon.com/s/dm/optimization/server-side-test/sem-generic/free-b/?sc_channel=PS&sc_campaign=acquisition_US&sc_publisher=google&sc_medium=cloud_computing_hv_b&sc_content=aws_core_e_test_q32016&sc_detail=amazon%2520-%2520aws&sc_category=cloud_computing&sc_segment=102882732282&sc_matchtype=e&sc_country=US&s_kwcid=AL!4422!3!102882732282!e!!g!!amazon%2520-%2520aws&ef_id=WHrLLwAABAl9uhYF:20170120011018:s">Amazon AWS</a>, everything in this
experiment was conducted just on this one laptop.  While better
hardware would be almost certainly be essential for real Deep
Learning applications and autonomous vehicles, in this toy problem
it wasn't really necessary.
</p>

<p>
Also, note that everything in this project was done in this
document's corresponding <a href="README.html">README.org</a> Org-Mode file in Emacs.  This
file does not merely document the code.  This document <i>is</i> the
code.  Like <a href="http://jupyter.org/">Jupyter</a> notebooks, it is an example of
<a href="https://en.wikipedia.org/wiki/Literate_programming">literate
programming</a>, and the <a href="model.py">model.py</a> file is <i>generated</i> from this
document.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Methods</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Data</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1">Collection and Preparation</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
Behavioral cloning relies on training neural networks with data
exhibiting the very behavior you wish to clone.  One way to
achieve that for this project is to use a driving simulator
provided by Udacity, which in its "training mode" can emit a
stream of data samples as the user operates the car.  Each
sample consists of a triplet of images and a single floating
point number in the interval [-1, 1], recording the view and the
steering angle for the simulation and car at regular intervals.
The three images are meant to be from three "cameras" mounted on
the simulated car's left, center, and right, giving three
different aspects of the scene and in principle providing
stereoscopic depth information.
</p>

<p>
The driving simulator also has an "autonomous mode" in which the
car interacts with a network server to exchange telemetry that
guides the car.  The simulator sends the network server camera
images and the network server is expected to reply with steering
angles.  So, not only is the driving simulator critical for
understanding the problem and helpful for obtaining training
data, it is absolutely essential for evaluating the solution.
</p>

<p>
Actually, Udacity provides not <a href="https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f0f7_simulator-linux/simulator-linux.zip">one</a> but <a href="https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587527cb_udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5/udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5.zip">two</a> simulators.  The
first is the stock simulator and the second is an enhanced
simulator, whose ability to use a computer mouse as input is
very important for acquiring good training data with smoothly
varying steering angles.  So, why not get both?  Here, we
download and unzip the Linux versions into sub-directories
<code>simulator-linux</code> and <code>simulator-beta</code>.
</p>

<div class="org-src-container">

<pre class="src src-sh">wget -O simulator-linux.zip <span style="color: #e9b96e;">"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5831f0f7_simulator-linux/simulator-linux.zip"</span>
wget -O simulator-beta.zip <span style="color: #e9b96e;">"https://d17h27t6h515a5.cloudfront.net/topher/2017/January/587527cb_udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5/udacity-sdc-udacity-self-driving-car-simulator-dominique-development-linux-desktop-64-bit-5.zip"</span>
unzip -d simulator-linux -u simulator-linux.zip &gt; /dev/null 2&gt;&amp;1
unzip -d simulator-beta -u simulator-beta.zip &gt; /dev/null 2&gt;&amp;1
</pre>
</div>

<p>
While we are at it, we might as well get the network server as
well, which is implemented in the <a href="https://d17h27t6h515a5.cloudfront.net/topher/2017/January/586c4a66_drive/drive.py">drive.py</a> Python file.
</p>

<div class="org-src-container">

<pre class="src src-sh">wget https://d17h27t6h515a5.cloudfront.net/topher/2017/January/586c4a66_drive/drive.py
</pre>
</div>

<p>
Now, while we are encouraged to collect our own training data,
it turns out that Udacity supplies their own training data for
the first of the two tracks, which is the track on which the
solution will be validated.  We might as well get that as well,
and see how much progress we can make just with the provided
samples.  The data are in a zip file, <a href="https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip">data.zip</a>, which we of
course unzip, taking care to remove the annoying <code>__MACOSX</code>
directory.
</p>

<div class="org-src-container">

<pre class="src src-sh">wget -nc <span style="color: #e9b96e;">"https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip"</span>
unzip data.zip &gt; /dev/null 2&gt;&amp;1
rm -rf __MACOSX
</pre>
</div>

<p>
The data&#x2014;whether recorded or downloaded&#x2014;are presented as a
<a href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> "index file", <code>driving_log.csv</code>.  Each line in this file
correlates images with the steering angle, throttle, brake, and
speed of the car.  The images are related via filenames in the
first three fields, which refer to the center, left, and right
camera images stored in files in the <code>IMG</code> subdirectory.  We can
take a look at the beginning of that file and then determine how
many samples are provided.
</p>

<div class="org-src-container">

<pre class="src src-sh">head data/driving_log.csv
wc -l data/driving_log.csv
</pre>
</div>

<pre class="example">
center,left,right,steering,throttle,brake,speed
IMG/center_2016_12_01_13_30_48_287.jpg, IMG/left_2016_12_01_13_30_48_287.jpg, IMG/right_2016_12_01_13_30_48_287.jpg, 0, 0, 0, 22.14829
IMG/center_2016_12_01_13_30_48_404.jpg, IMG/left_2016_12_01_13_30_48_404.jpg, IMG/right_2016_12_01_13_30_48_404.jpg, 0, 0, 0, 21.87963
IMG/center_2016_12_01_13_31_12_937.jpg, IMG/left_2016_12_01_13_31_12_937.jpg, IMG/right_2016_12_01_13_31_12_937.jpg, 0, 0, 0, 1.453011
IMG/center_2016_12_01_13_31_13_037.jpg, IMG/left_2016_12_01_13_31_13_037.jpg, IMG/right_2016_12_01_13_31_13_037.jpg, 0, 0, 0, 1.438419
IMG/center_2016_12_01_13_31_13_177.jpg, IMG/left_2016_12_01_13_31_13_177.jpg, IMG/right_2016_12_01_13_31_13_177.jpg, 0, 0, 0, 1.418236
IMG/center_2016_12_01_13_31_13_279.jpg, IMG/left_2016_12_01_13_31_13_279.jpg, IMG/right_2016_12_01_13_31_13_279.jpg, 0, 0, 0, 1.403993
IMG/center_2016_12_01_13_31_13_381.jpg, IMG/left_2016_12_01_13_31_13_381.jpg, IMG/right_2016_12_01_13_31_13_381.jpg, 0, 0, 0, 1.389892
IMG/center_2016_12_01_13_31_13_482.jpg, IMG/left_2016_12_01_13_31_13_482.jpg, IMG/right_2016_12_01_13_31_13_482.jpg, 0, 0, 0, 1.375934
IMG/center_2016_12_01_13_31_13_584.jpg, IMG/left_2016_12_01_13_31_13_584.jpg, IMG/right_2016_12_01_13_31_13_584.jpg, 0, 0, 0, 1.362115
8037 data/driving_log.csv
</pre>

<p>
We have 8037 lines, but descriptive labels are provided in the
first line of this file.  We strip that line out.  Also, Deep
Learning lore says that it is often prudent to randomize the
data when possible and always prudent to split the data into
training and validation sets.  Here we do all three in just a
few lines of shell code, taking 1000 samples (about 12%) as
validation data.
</p>

<div class="org-src-container">

<pre class="src src-sh">cat data/driving_log.csv | tail -n+2 | shuf &gt; data/driving_log_all.csv
cat data/driving_log_all.csv | head -n1000 &gt; data/driving_log_validation.csv
cat data/driving_log_all.csv | tail -n+1001 &gt; data/driving_log_train.csv
</pre>
</div>

<p>
As a sanity check, we report the number of total samples,
training samples, and validation samples.  Even if the provided
Udacity data are insufficient ultimately for delivering a
solution, they are valuable for establishing a baseline for
developing that solution.  A project like this has many free
parameters and the combinatorial explosion among them can
quickly overwhelm the researcher, and so eliminating some of
those free parameters by avoiding recording one's own data&#x2014;if
only in the interim&#x2014;is a real boon.  We do not use the
<a href="data/driving_log_all.csv"><code>driving_log_all.csv</code></a> file after this point; it served as a
handy placeholder of the original data, shuffled and with the
header removed.  However, the <a href="data/driving_log_train.csv"><code>driving_log_train.csv</code></a> and
<a href="data/driving_log_validation.csv"><code>driving_log_validation.csv</code></a> files play a central role as we
iteratively develop and refine the model.
</p>

<div class="org-src-container">

<pre class="src src-sh">wc -l data/driving_log_all.csv
wc -l data/driving_log_train.csv
wc -l data/driving_log_validation.csv
</pre>
</div>

<pre class="example">
8036 data/driving_log_all.csv
7036 data/driving_log_train.csv
1000 data/driving_log_validation.csv
</pre>

<p>
Before leaving the land of shell commands for the land of Python
scripts and neural nets, we create one other useful data file.
Paul Heraty <a href="https://carnd-forums.udacity.com/questions/26214464/behavioral-cloning-cheatsheet">argues</a> that it can be useful in the early stages of
developing a solution to "overtrain" it on a small sample
comprising disparate canonical examples.  As we shall see, we
can confirm that this was <i>extremely</i> good advice.
</p>

<p>
One of the chief difficulties we encountered as a newcomer to Deep
Learning and its community of tools was simply "getting it to
work in the first place," independent of whether the model
actually was very good.  One of the chief strategies for
overcoming this difficulty we found is to "try to get a pulse:"
develop the basic machinery of the model and solution first,
with little or no regard for its fidelity.  Working through the
inevitable blizzard of error messages one first encounters is no
small task.  Once it is cleared and the practitioner has
confidence his tools are working well, then it becomes possible
to rapidly iterate and converge to a <i>good</i> solution.  
</p>

<p>
Creating an "overtraining sample" is good because overtraining
is a vivid expectation that can quickly be realized (especially
with only 3 samples), and if overtraining does not occur you
know you have deeper problems.
</p>

<p>
With a little magic from <a href="https://www.gnu.org/software/bash/manual/">Bash</a>, <a href="https://www.gnu.org/software/gawk/manual/gawk.html">Awk</a>, etc., we can select three
disparate samples, with neutral steering, extreme left steering,
and extreme right steering.
</p>

<div class="org-src-container">

<pre class="src src-sh">cat &lt;(cat data/driving_log_all.csv | sort -k4 -n -t, | head -n1) &lt;(cat data/driving_log_all.csv | sort -k4 -nr -t, | head -n1) &lt;(cat data/driving_log_all.csv | awk -F, -vOFS=, <span style="color: #e9b96e;">'{print $1, $2, $3, sqrt($4*$4), $5, $6, $7}'</span> | sort -k4 -n -t, | head -n1) &gt; data/driving_log_overtrain.csv
cat data/driving_log_overtrain.csv
</pre>
</div>

<pre class="example">
IMG/center_2016_12_01_13_39_28_024.jpg, IMG/left_2016_12_01_13_39_28_024.jpg, IMG/right_2016_12_01_13_39_28_024.jpg, -0.9426954, 0, 0, 28.11522
IMG/center_2016_12_01_13_38_46_752.jpg, IMG/left_2016_12_01_13_38_46_752.jpg, IMG/right_2016_12_01_13_38_46_752.jpg, 1, 0, 0, 13.2427
IMG/center_2016_12_01_13_30_48_287.jpg, IMG/left_2016_12_01_13_30_48_287.jpg, IMG/right_2016_12_01_13_30_48_287.jpg,0, 0, 0, 22.14829
</pre>

<p>
We will be able to see exactly what the images for these samples
once we set up a suitable Python environment, which we do in the
next section.
</p>
</div>
</div>

<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2">Setup</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
Udacity helpfully provides a <a href="https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/83ec35ee-1e02-48a5-bdb7-d244bd47c2dc/lessons/8c82408b-a217-4d09-b81d-1bda4c6380ef/concepts/4f1870e0-3849-43e4-b670-12e6f2d4b7a7">CarND Starter Kit</a> in the contents
of a particular <a href="https://github.com/udacity/CarND-Term1-Starter-Kit/blob/master/README.md">GitHub repository</a>.  The instructions there lead
one to create a <a href="http://conda.pydata.org/docs/">Conda</a> environment for Python.  We did that and
extracted its definition into the <a href="environment.yml">environment.yml</a> file.  That
way, if the environment does not already exist it can be created
and activated with these commands.
</p>

<div class="org-src-container">

<pre class="src src-sh">conda env create --file environment.yml --name CarND-Behavioral-Cloning
<span style="color: #e090d7;">source</span> activate CarND-Behavioral-Cloning
</pre>
</div>

<p>
Having set up the Conda environment, and activated it, now we
can finally load the Python modules that we will need in later
sections.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">from</span> PIL <span style="color: #b4fa70;">import</span> Image
<span style="color: #b4fa70;">from</span> itertools <span style="color: #b4fa70;">import</span> groupby, islice, zip_longest, cycle, filterfalse
<span style="color: #b4fa70;">from</span> keras.layers <span style="color: #b4fa70;">import</span> Conv2D, Flatten, MaxPooling2D, Dense, Dropout, Lambda, AveragePooling2D
<span style="color: #b4fa70;">from</span> keras.layers.convolutional <span style="color: #b4fa70;">import</span> Cropping2D, Convolution2D
<span style="color: #b4fa70;">from</span> keras.models <span style="color: #b4fa70;">import</span> Sequential, model_from_json
<span style="color: #b4fa70;">from</span> keras.utils.visualize_util <span style="color: #b4fa70;">import</span> plot
<span style="color: #b4fa70;">from</span> scipy.stats <span style="color: #b4fa70;">import</span> kurtosis, skew, describe
<span style="color: #b4fa70;">import</span> matplotlib.pyplot <span style="color: #b4fa70;">as</span> plt
<span style="color: #b4fa70;">import</span> numpy <span style="color: #b4fa70;">as</span> np
<span style="color: #b4fa70;">import</span> pprint <span style="color: #b4fa70;">as</span> pp
<span style="color: #b4fa70;">import</span> random
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-1-3" class="outline-4">
<h4 id="sec-2-1-3">Utilities</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
Another piece of advice impressed upon students in the class, to
the point of it practically being a requirement, was to learn to
use Python <a href="https://wiki.python.org/moin/Generators">generators</a> and the <a href="https://keras.io/models/sequential/"><code>fit_generator</code></a> function in our
Deep Learning toolkit, <a href="https://keras.io/">Keras</a>.  Generators allow for a form of
<a href="https://en.wikipedia.org/wiki/Lazy_loading">lazy loading</a>, which can be useful in Machine Learning settings
where large data sets that do not fit into main memory are the
norm.  Keras makes use of that with <code>fit_generator</code>, which
expects input presented as generators that infinitely recycle
over the underlying data.
</p>

<p>
I took that advice to heart and spent considerable
time&#x2014;perhaps more than was necessary&#x2014;learning about
generators and generator expressions.  It did pay off somewhat
in that I developed a tiny library of reusable, composeable
generator expressions, which are presented here.
</p>

<p>
Before doing that, though, first a detour and a bit of advice.
Anyone who is working with Python generators is urged to become
acquainted with <a href="https://docs.python.org/3/library/itertools.html"><code>itertools</code></a>, a standard Python library of
reusable, composeable generators.  For me the <a href="https://docs.python.org/3/library/itertools.html#itertools.cycle"><code>cycle</code></a> generator
was a key find.  As mentioned above, <code>fit_generator</code> needs
infinitely-recycling generators, which is exactly what
<code>itertools.cycle</code> provides.  One wrinkle is that <code>cycle</code>
accomplishes this with an internal data cache, so if your data
do <i>not</i> fit in memory you may have to seek an alternative.
However, if your data <i>do</i> fit in memory this confers a very
nice property, for free: after cycling through the data the
first time all subsequent retrievals are from memory, so that
performance improves dramatically after the first cycle.
</p>

<p>
This turns out to be very beneficial and entirely appropriate
for our problem.  Suppose we use the Udacity data.  In that
case, we have 8136 images (training + validation) provided we
use one camera only (such as the center camera).  As we shall
see below, each image is a 320x160 pixel array of RGB values,
for a total of 150k per image.  That means approximately 1 GB of
RAM is required to store the Udacity data.  My 4 year-old laptop
has 4 times that.  Now, this rosy picture might quickly dissolve
if we use much more data, such as by using the other camera
angles and/or acquiring more training data.  Then again, it may
not.  With virtual memory, excess pages <i>should</i> be swapped out
to disk.  That's not ideal, but to first order it's not obvious
that it's functionally much different from or much worse than
recycling the data by repeatedly loading the raw image files.
In fact, it may be better, since at least we only perform the
actual translation from PNG format to <a href="http://www.numpy.org/">NumPy</a> data arrays once for
each image.
</p>

<p>
If we are really concerned about memory usage we might consider
reducing the input image size, such as with OpenCV's
<a href="http://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#resize"><code>cv2.resize</code></a> command, and we might consider cropping the image.
But, I think we should think carefully about this.  These
operations may have an effect on the performance of the model,
and so manipulating the images in this way is not something we
should take lightly.  Nevertheless, it can be beneficial as we
shall see shortly.  However, if we <i>do</i> decide to crop and
resize, there is a technical trade-off to be made.  Either we
can crop and resize as a pre-processing step, or we can do it
directly within the model, and there are advantages and
disadvantages to each.  If we crop and resize as a
pre-processing step, it has direct impact on the aforementioned
memory considerations.  But, we <i>must take care to perform exactly the same crop and resize operations in the network server!</i>  Since cropping and resizing essentially introduce new
hyper-parameters, those parameters somehow must be communicated
to <code>drive.py</code>.  If we crop and resize directly within the model,
it has no beneficial impact on the aforementioned memory
considerations.  But, <i>we get those operations and their internal hyper-parameters for free within the network server!</i>  I found
the latter advantage to be much greater and so the trade-off I
selected was to crop and resize within the model.
</p>

<p>
In any case, my experiments showed that for the Udacity data at
least, loading the data into an in-memory cache via
<code>itertools.cycle</code> (or, more precisely, my variation of it) and
then infinitely recycling over them proved to be a very good
solution.
</p>

<p>
However, there is one problem with <code>itertools.cycle</code> by itself,
and that is that again, according to Deep Learning lore, it is
prudent to randomize the data on every epoch.  To do that, we
need to rewrite <code>itertools.cycle</code> so that it shuffles the data
upon every recycle.  That is easily done, as shown below.  Note
that the elements of the iterable are essentially returned in
batches, and that the first batch is not shuffled.  If you want
only to return random elements then you must know the batch
size, which will be the number of elements in the underlying
finite iterable, and you must discard the first batch.  The
itertools.islice function can be helpful here.  In our case it
is not a problem since all of the data were already shuffled
once using Unix command-line utilities above.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">rcycle</span>(iterable):
    <span style="color: #fcaf3e;">saved</span> = []                 <span style="color: #73d216;"># </span><span style="color: #73d216;">In-memory cache</span>
    <span style="color: #b4fa70;">for</span> element <span style="color: #b4fa70;">in</span> iterable:
        <span style="color: #b4fa70;">yield</span> element
        saved.append(element)
    <span style="color: #b4fa70;">while</span> saved:
        random.shuffle(saved)  <span style="color: #73d216;"># </span><span style="color: #73d216;">Shuffle every batch</span>
        <span style="color: #b4fa70;">for</span> element <span style="color: #b4fa70;">in</span> saved:
              <span style="color: #b4fa70;">yield</span> element
</pre>
</div>

<p>
If we invoke <code>rcycle</code> on a sequence drawn from the interval
[0,5), taken in 3 batches for a total of 15 values we can see
this behavior.  The first 5 values are drawn in order, but then
the next 10 are drawn in two batches, each batch shuffled
independently.  In practice, this is not a problem.
</p>

<div class="org-src-container">

<pre class="src src-python">[x <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> islice(rcycle(<span style="color: #e090d7;">range</span>(5)), 15)]
</pre>
</div>

<pre class="example">
[0, 1, 2, 3, 4, 1, 3, 4, 2, 0, 3, 1, 4, 0, 2]
</pre>

<p>
The remaining utility functions that I wrote are quite
straightforward and for brevity are written as "one-liners."
</p>

<dl class="org-dl">
<dt> feed </dt><dd>generator that feeds lines from the file named by 'filename'
</dd>
<dt> split </dt><dd>generator that splits lines into tuples based on a delimiter
</dd>
<dt> select </dt><dd>generator that selects out elements from tuples
</dd>
<dt> load </dt><dd>non-generator that reads an image file into a NumPy array
</dd>
<dt> Xflip </dt><dd>non-generator that flips an input image horizontally
</dd>
<dt> yflip </dt><dd>non-generator that flips a target label to its negative
</dd>
<dt> rmap </dt><dd>generator that randomly applies or does not apply a
function with equal probability
</dd>
<dt> rflip </dt><dd>generator that flips samples 50% of the time
</dd>
<dt> fetch </dt><dd>generator that loads index file entries into samples
</dd>
<dt> group </dt><dd>generator that groups input elements into lists
</dd>
<dt> transpose </dt><dd>generator that takes a generator of lists into a
list of generators
</dd>
<dt> batch </dt><dd>generator that takes a list of generators into a list
of NumPy array "batches"
</dd>
</dl>

<p>
The actual code for these utility functions is presented below.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">feed</span> = <span style="color: #b4fa70;">lambda</span> filename: (l <span style="color: #b4fa70;">for</span> l <span style="color: #b4fa70;">in</span> <span style="color: #e090d7;">open</span>(filename))
<span style="color: #fcaf3e;">split</span> = <span style="color: #b4fa70;">lambda</span> <span style="color: #fcaf3e;">lines</span>, <span style="color: #fcaf3e;">delimiter</span>=<span style="color: #e9b96e;">","</span>: (line.split(delimiter) <span style="color: #b4fa70;">for</span> line <span style="color: #b4fa70;">in</span> lines)
<span style="color: #fcaf3e;">select</span> = <span style="color: #b4fa70;">lambda</span> fields, indices: ([r[i] <span style="color: #b4fa70;">for</span> i <span style="color: #b4fa70;">in</span> indices] <span style="color: #b4fa70;">for</span> r <span style="color: #b4fa70;">in</span> fields)
<span style="color: #fcaf3e;">load</span> = <span style="color: #b4fa70;">lambda</span> f: np.asarray(Image.<span style="color: #e090d7;">open</span>(f))
<span style="color: #fcaf3e;">Xflip</span> = <span style="color: #b4fa70;">lambda</span> x: x[:,::-1,:]
<span style="color: #fcaf3e;">yflip</span> = <span style="color: #b4fa70;">lambda</span> x: -x
<span style="color: #fcaf3e;">sflip</span> = <span style="color: #b4fa70;">lambda</span> s: (Xflip(s[0]), yflip(s[1]))
<span style="color: #fcaf3e;">rmap</span> = <span style="color: #b4fa70;">lambda</span> f,g: (x <span style="color: #b4fa70;">if</span> random.choice([<span style="color: #e9b2e3;">True</span>, <span style="color: #e9b2e3;">False</span>]) <span style="color: #b4fa70;">else</span> f(x) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> g)
<span style="color: #fcaf3e;">rflip</span> = <span style="color: #b4fa70;">lambda</span> s: rmap(sflip, s)
<span style="color: #fcaf3e;">fetch</span> = <span style="color: #b4fa70;">lambda</span> records, base: ([load(base+f.strip()) <span style="color: #b4fa70;">for</span> f <span style="color: #b4fa70;">in</span> record[:1]]+[<span style="color: #e090d7;">float</span>(v) <span style="color: #b4fa70;">for</span> v <span style="color: #b4fa70;">in</span> record[1:]] <span style="color: #b4fa70;">for</span> record <span style="color: #b4fa70;">in</span> records)
<span style="color: #fcaf3e;">group</span> = <span style="color: #b4fa70;">lambda</span> <span style="color: #fcaf3e;">items</span>, <span style="color: #fcaf3e;">n</span>, <span style="color: #fcaf3e;">fillvalue</span>=<span style="color: #e9b2e3;">None</span>: zip_longest(*([<span style="color: #e090d7;">iter</span>(items)]*n), fillvalue=fillvalue)
<span style="color: #fcaf3e;">transpose</span> = <span style="color: #b4fa70;">lambda</span> tuples: (<span style="color: #e090d7;">list</span>(<span style="color: #e090d7;">map</span>(<span style="color: #e090d7;">list</span>, <span style="color: #e090d7;">zip</span>(*g))) <span style="color: #b4fa70;">for</span> g <span style="color: #b4fa70;">in</span> tuples)
<span style="color: #fcaf3e;">batch</span> = <span style="color: #b4fa70;">lambda</span> <span style="color: #fcaf3e;">groups</span>, <span style="color: #fcaf3e;">indices</span>=[0, 1]: ([np.asarray(t[i]) <span style="color: #b4fa70;">for</span> i <span style="color: #b4fa70;">in</span> indices] <span style="color: #b4fa70;">for</span> t <span style="color: #b4fa70;">in</span> groups)
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-1-4" class="outline-4">
<h4 id="sec-2-1-4">Exploratory Analysis</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
It often pays to explore your data with relatively few
constraints before diving in to build and train the actual
model.  One may gain insights that help guide you to better
models and strategies, and avoid pitfalls and dead-ends.  
</p>

<p>
To that end, first we just want to see what kind of input data
we are dealing with.  We know that they are RGB images, so let's
load a few of them for display.  Here, we show the three frames
taken from the <code>driving_log_overtrain.csv</code> file described
above&#x2014;center camera only&#x2014;labeled by their corresponding
steering angles.  As you can see, the image with a large
negative angle seems to have the car on the extreme right edge
of the road.  Perhaps the driver in this situation was executing
a "recovery" maneuver, turning sharply to the left to veer away
from the road's right edge and back to the centerline.
Likewise, with the next figure that has a large positive angle,
we see that the car appears to be on the extreme left edge of
the road.  Perhaps the opposite recovery maneuver was in play.
Finally, in the third and last image that has a neutral steering
angle (0.0), the car appears to be sailing right down the middle
of the road, a circumstance that absent extraneous circumstances
(other cars, people, rodents) should not require corrective
steering.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">X</span> = [x <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> fetch(select(split(feed(<span style="color: #e9b96e;">"data/driving_log_overtrain.csv"</span>)), [0,3]), <span style="color: #e9b96e;">"data/"</span>)]
<span style="color: #fcaf3e;">f</span> = plt.figure()                        <span style="color: #73d216;"># </span><span style="color: #73d216;">start a figure</span>
plt.imshow(X[0][0])                     <span style="color: #73d216;"># </span><span style="color: #73d216;">show the image</span>
f.suptitle(<span style="color: #e9b96e;">"Angle: "</span> + <span style="color: #e090d7;">str</span>(X[0][1]))    <span style="color: #73d216;"># </span><span style="color: #73d216;">add figure title</span>
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road1.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #fcaf3e;">f</span> = plt.figure()
plt.imshow(X[1][0])
f.suptitle(<span style="color: #e9b96e;">"Angle: "</span> + <span style="color: #e090d7;">str</span>(X[1][1]))
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road2.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #fcaf3e;">f</span> = plt.figure()
plt.imshow(X[2][0])
f.suptitle(<span style="color: #e9b96e;">"Angle: "</span> + <span style="color: #e090d7;">str</span>(X[2][1]))
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road3.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
</pre>
</div>


<div class="figure">
<p><img src="road1.png" alt="road1.png" />
</p>
<p><span class="figure-number">Figure 3:</span> Large Negative Steering Angle</p>
</div>


<div class="figure">
<p><img src="road2.png" alt="road2.png" />
</p>
<p><span class="figure-number">Figure 4:</span> Large Positive Steering Angle</p>
</div>


<div class="figure">
<p><img src="road3.png" alt="road3.png" />
</p>
<p><span class="figure-number">Figure 5:</span> Neutral Steering Angle</p>
</div>

<p>
Next, we get the shape of an image which, as we said above, is
320x160x3.  In NumPy parlance that's <code>(160, 320, 3)</code>, for 160
rows (the y direction), 320 columns (the x direction), and 3
channels (the RGB colorspace).
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">print</span>(X[0][0].shape)  <span style="color: #73d216;"># </span><span style="color: #73d216;">X[0] is an (image,angle) sample. X[0][0] is just the image</span>
</pre>
</div>

<pre class="example">
(160, 320, 3)
</pre>

<p>
We can see that the images naturally divide roughly into "road"
below the horizon and "sky" above the horizon, with background
scenery (trees, mountains, etc.) superimposed onto the sky.
While the sky (really, the scenery) might contain useful
navigational information, it is plausible that it contains
little or no useful information for the simpler task of
maintaining an autonomous vehicle near the centerline of a
track, a subject we shall return to later.  Likewise, it is
almost certain that the small amount of car "hood" superimposed
onto the bottom of the images contains no useful information.
Therefore, let us see what the images would look like with the
hood cropped out on the bottom by 20 pixels, and the sky cropped
out on the top by <a href="#coderef-sky60"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sky60');" onmouseout="CodeHighlightOff(this, 'coderef-sky60');">60 pixels</a>, <a href="#coderef-sky80"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sky80');" onmouseout="CodeHighlightOff(this, 'coderef-sky80');">80 pixels</a>, and <a href="#coderef-sky100"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sky100');" onmouseout="CodeHighlightOff(this, 'coderef-sky100');">100 pixels</a>.  
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">f</span> = plt.figure()
<span id="coderef-sky60" class="coderef-off">plt.imshow(X[0][0][60:140])    <span style="color: #73d216;"># </span><span style="color: #73d216;">sky:60</span></span>
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road4.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span id="coderef-sky80" class="coderef-off">plt.imshow(X[0][0][80:140])    <span style="color: #73d216;"># </span><span style="color: #73d216;">sky:80</span></span>
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road5.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span id="coderef-sky100" class="coderef-off">plt.imshow(X[0][0][100:140])   <span style="color: #73d216;"># </span><span style="color: #73d216;">sky:100</span></span>
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road6.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
</pre>
</div>


<div class="figure">
<p><img src="road4.png" alt="road4.png" />
</p>
<p><span class="figure-number">Figure 6:</span> Hood Crop: 20, Sky Crop:  60</p>
</div>


<div class="figure">
<p><img src="road5.png" alt="road5.png" />
</p>
<p><span class="figure-number">Figure 7:</span> Hood Crop: 20, Sky Crop:  80</p>
</div>


<div class="figure">
<p><img src="road6.png" alt="road6.png" />
</p>
<p><span class="figure-number">Figure 8:</span> Hood Crop: 20, Sky Crop:  100</p>
</div>

<p>
I should pause here to address the issue of why we are only
using the center camera.  After all, the training data do
provide two additional camera images: the left and right
cameras.  Surely, those provide additional useful information
that the model potentially could make use of.  However, in my
opinion there is a serious problem in using these data: the
simulator seems only to send to the network server in
<code>drive.py</code> the center image.  I really do not understand why
this is the case, since obviously the simulator is fully-capable
of scribbling the extra camera outputs down when recording
training data.  
</p>

<p>
Now, I have observed considerable discussion on the Slack
channel for this course on the subject of using the additional
camera images (left and right) along with a corresponding shift
of the steering angles.  Frankly, I am skeptical about the merit
of this strategy and shall avoid this practice, evaluate the
outcome, and adopt it only if absolutely necessary.
</p>

<p>
We begin by conducting a very simple analysis of the target
labels, which again are steering angles in the interval [-1,
1].  In fact, as real-valued outputs it may be a stretch to call
them "labels" and this is not really a classification problem.
Nevertheless in the interest of time we will adopt the term.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">f</span> = plt.figure()                   
<span style="color: #fcaf3e;">y1</span> = np.array([<span style="color: #e090d7;">float</span>(s[0]) <span style="color: #b4fa70;">for</span> s <span style="color: #b4fa70;">in</span> select(split(feed(<span style="color: #e9b96e;">"data/driving_log_all.csv"</span>)),[3])])
<span style="color: #fcaf3e;">h</span> = plt.hist(y1,bins=100)          <span style="color: #73d216;"># </span><span style="color: #73d216;">plot histogram</span>
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"hist1.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">'png'</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
pp.pprint(describe(y1)._asdict())  <span style="color: #73d216;"># </span><span style="color: #73d216;">print descriptive statistics</span>
</pre>
</div>

<pre class="example">
&gt;&gt;&gt;
OrderedDict([('nobs', 8036),
             ('minmax', (-0.94269539999999996, 1.0)),
             ('mean', 0.0040696440648332506),
             ('variance', 0.016599764281272529),
             ('skewness', -0.1302892457752191),
             ('kurtosis', 6.311554102057668)])
</pre>


<div class="figure">
<p><img src="hist1.png" alt="hist1.png" />
</p>
<p><span class="figure-number">Figure 9:</span> All Samples - No Reflection</p>
</div>

<p>
The data have non-zero <i>mean</i> and <i>skewness</i>, perhaps arising
from a bias toward left-hand turns when driving on a closed
track.
</p>

<p>
The data are dominated by small steering angles because the car
spends most of its time on the track in straightaways.  The
asymmetry in the data is more apparent if I mask out small
angles and repeat the analysis.  Steering angles occupy the
interval [-1, 1], but the "straight" samples appear to be within
the neighborhood [-0.01, 0.01].
</p>

<p>
We might consider masking out small angled samples from the
actual training data as well, a subject we shall return to in
the summary.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">f</span> = plt.figure()
<span style="color: #fcaf3e;">p</span> = <span style="color: #b4fa70;">lambda</span> x: <span style="color: #e090d7;">abs</span>(x)&lt;0.01
<span style="color: #fcaf3e;">y2</span> = np.array([s <span style="color: #b4fa70;">for</span> s <span style="color: #b4fa70;">in</span> filterfalse(p,y1)])
<span style="color: #fcaf3e;">h</span> = plt.hist(y2,bins=100)
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"hist2.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">'png'</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
pp.pprint(describe(y2)._asdict())
</pre>
</div>

<pre class="example">
&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;
OrderedDict([('nobs', 3584),
             ('minmax', (-0.94269539999999996, 1.0)),
             ('mean', 0.0091718659514508933),
             ('variance', 0.037178302717086116),
             ('skewness', -0.16657825969015194),
             ('kurtosis', 1.1768785967587378)])
</pre>


<div class="figure">
<p><img src="hist2.png" alt="hist2.png" />
</p>
<p><span class="figure-number">Figure 10:</span> abs(angle)&gt;0.01 - No Reflection</p>
</div>

<p>
A simple trick we can play to remove this asymmetry&#x2014;if we
wish&#x2014;is to join the data with its reflection, effectively
doubling our sample size in the process.  For illustration
purposes only, we shall again mask out small angle samples.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">f</span> = plt.figure()
<span style="color: #fcaf3e;">y3</span> = np.append(y2, -y2)
<span style="color: #fcaf3e;">h</span> = plt.hist(y3,bins=100)
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"hist3.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">'png'</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
pp.pprint(describe(y3)._asdict())
</pre>
</div>

<pre class="example">
&gt;&gt;&gt; &gt;&gt;&gt;
OrderedDict([('nobs', 7168),
             ('minmax', (-1.0, 1.0)),
             ('mean', 0.0),
             ('variance', 0.03725725015081123),
             ('skewness', 0.0),
             ('kurtosis', 1.1400026599654964)])
</pre>


<div class="figure">
<p><img src="hist3.png" alt="hist3.png" />
</p>
<p><span class="figure-number">Figure 11:</span> abs(angle)&gt;0.01 - Full Reflection</p>
</div>

<p>
In one of the least-surprising outcomes of the year, after
performing the reflection and joining operations, the data now
are symmetrical, with mean and skewness identically 0.
</p>

<p>
Of course, in this analysis I have only reflected the target
labels.  If I apply this strategy to the training data,
naturally I need to reflect along their horizontal axes the
corresponding input images as well.  In fact, that is the
purpose of the <code>Xflip</code>, <code>yflip</code>, <code>rmap</code>, <code>rflip</code>, and <code>sflip</code>
utility functions described above.
</p>

<p>
It turns out there is another approach to dealing with the bias
and asymmetry in the training data.  In lieu of reflecting the
data, which by definition imposes a 0 mean and 0 skewness, we
can instead just randomly flip samples 50% of the time.  While
that will not yield a perfectly balanced and symmetric data
distribution, given enough samples it should give us a crude
approximation.  Moreover, it saves us from having to store more
images in memory, at the cost of some extra computation.
Essentially, we are making the classic space-time trade-off
between memory consumption and CPU usage.  
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">y4</span> = [y <span style="color: #b4fa70;">for</span> y <span style="color: #b4fa70;">in</span> rmap(yflip, islice(cycle(y2), 2*y1.shape[0]))]  <span style="color: #73d216;"># </span><span style="color: #73d216;">2 batches</span>
<span style="color: #fcaf3e;">y5</span> = [y <span style="color: #b4fa70;">for</span> y <span style="color: #b4fa70;">in</span> rmap(yflip, islice(cycle(y2), 4*y1.shape[0]))]  <span style="color: #73d216;"># </span><span style="color: #73d216;">4 batches</span>
<span style="color: #fcaf3e;">y6</span> = [y <span style="color: #b4fa70;">for</span> y <span style="color: #b4fa70;">in</span> rmap(yflip, islice(cycle(y2), 8*y1.shape[0]))]  <span style="color: #73d216;"># </span><span style="color: #73d216;">8 batches</span>
<span style="color: #fcaf3e;">f</span> = plt.figure()
<span style="color: #fcaf3e;">h</span> = plt.hist(y4,bins=100)
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"hist5.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">'png'</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #fcaf3e;">f</span> = plt.figure()
<span style="color: #fcaf3e;">h</span> = plt.hist(y5,bins=100)
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"hist6.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">'png'</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #fcaf3e;">f</span> = plt.figure()
<span style="color: #fcaf3e;">h</span> = plt.hist(y6,bins=100)
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"hist7.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">'png'</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
pp.pprint(describe(y4)._asdict())
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
pp.pprint(describe(y5)._asdict())
<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
pp.pprint(describe(y6)._asdict())
</pre>
</div>

<pre class="example">
&gt;&gt;&gt;
OrderedDict([('nobs', 16072),
	     ('minmax', (-1.0, 1.0)),
	     ('mean', -0.00052772518417122953),
	     ('variance', 0.037283229390251714),
	     ('skewness', 0.008520498589019836),
	     ('kurtosis', 1.1769644267914074)])

OrderedDict([('nobs', 32144),
	     ('minmax', (-1.0, 1.0)),
	     ('mean', -0.0022488201157292191),
	     ('variance', 0.037218482869856698),
	     ('skewness', -0.050201681486988635),
	     ('kurtosis', 1.1379036271452696)])

OrderedDict([('nobs', 64288),
	     ('minmax', (-1.0, 1.0)),
	     ('mean', -0.00011066913374191124),
	     ('variance', 0.037250831344353398),
	     ('skewness', -0.01390556638239454),
	     ('kurtosis', 1.1406077506908527)])
</pre>

<p>
Here, we see that as we increase the number of samples we draw
from the underlying data set, while randomly flipping them, the
mean tends to diminish.  The skewness does not behave quite so
well, though a coarser smoothing kernel (larger bin sizes for
the histograms) may help.  In any case, the following figures do
suggest that randomly flipping the data and drawing larger
sample sizes does help balance out the data.
</p>


<div class="figure">
<p><img src="hist5.png" alt="hist5.png" />
</p>
<p><span class="figure-number">Figure 12:</span> abs(angle)&gt;0.01 - Random Flipping, Recycle: 2</p>
</div>


<div class="figure">
<p><img src="hist6.png" alt="hist6.png" />
</p>
<p><span class="figure-number">Figure 13:</span> abs(angle)&gt;0.01 - Random Flipping, Recycle: 4</p>
</div>


<div class="figure">
<p><img src="hist7.png" alt="hist7.png" />
</p>
<p><span class="figure-number">Figure 14:</span> abs(angle)&gt;0.01 - Random Flipping, Recycle: 8</p>
</div>

<p>
The <code>sflip</code> utility function defined above flips not only the
target labels&#x2014;the steering angles&#x2014;but also the images (as it
must).  We check that by again displaying the 3 samples from
<code>driving_log_overtrain.csv</code> as above, but this time with each of
them flipped.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">X2</span> = [sflip(x) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> X]
<span style="color: #fcaf3e;">f</span> = plt.figure()                        
plt.imshow(X2[0][0])                     
f.suptitle(<span style="color: #e9b96e;">"Angle: "</span> + <span style="color: #e090d7;">str</span>(X2[0][1]))    
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road7.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #fcaf3e;">f</span> = plt.figure()
plt.imshow(X2[1][0])
f.suptitle(<span style="color: #e9b96e;">"Angle: "</span> + <span style="color: #e090d7;">str</span>(X2[1][1]))
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road8.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
<span style="color: #fcaf3e;">f</span> = plt.figure()
plt.imshow(X2[2][0])
f.suptitle(<span style="color: #e9b96e;">"Angle: "</span> + <span style="color: #e090d7;">str</span>(X2[2][1]))
<span style="color: #fcaf3e;">s</span> = plt.savefig(<span style="color: #e9b96e;">"road9.png"</span>, <span style="color: #e090d7;">format</span>=<span style="color: #e9b96e;">"png"</span>, bbox_inches=<span style="color: #e9b96e;">'tight'</span>)
</pre>
</div>


<div class="figure">
<p><img src="road7.png" alt="road7.png" />
</p>
<p><span class="figure-number">Figure 15:</span> Large Negative Steering Angle Flipped</p>
</div>


<div class="figure">
<p><img src="road8.png" alt="road8.png" />
</p>
<p><span class="figure-number">Figure 16:</span> Large Positive Steering Angle Flipped</p>
</div>


<div class="figure">
<p><img src="road9.png" alt="road9.png" />
</p>
<p><span class="figure-number">Figure 17:</span> Neutral Steering Angle Flipped</p>
</div>

<p>
If we compare these 3 figures depicting the flipped samples from
the <code>driving_log_overtrain.csv</code> set, with the original unflipped
samples in the figures above we can confirm the expected
results.  The images are indeed horizontally-reflected mirror
images, and the corresponding steering angles indeed have their
signs flipped (though, trivially for the neutral-steering
case).  
</p>

<p>
After that grueling ordeal, and armed with these results, we now
can develop a strategy for presenting training data to the
model.
</p>

<ol class="org-ol">
<li>Start with the Udacity data and see how far we can get.
</li>
<li>Split the data into training and validation files using
command-line tools.
</li>
<li>Use <code>itertools</code> and custom utilities, leaning heavily on
generator expressions.
</li>
<li>Take advantage of the small data sizes by keeping the
original data in memory.
</li>
<li>But, perform a crude "augmentation" by randomly flipping the
data horizontally (both images and steering angles) and
recycling the base data.
</li>
<li>Consider cropping the data to reduce the input sizes, reduce
memory usage, reduce the model size, and hopefully remove
superfluous information.
</li>
<li>Use only the center-camera data and see how far we can get. 
</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Implementation</h3>
<div class="outline-text-3" id="text-2-2">
<p>
There are many approaches to selecting or developing a model.  The
one I took was to assume that there is wisdom and experience
already embedded in the Deep Learning models that already have
been developed in the autonomous-vehicle community.  I decided to
choose one of those models as a starting point, and then build off
of it or adapt it as needed.  There are many to choose from, but
two well-known and often-used ones are a <a href="https://github.com/commaai/research/blob/master/train_steering_model.py">model</a> from <a href="http://comma.ai/">comma.ai</a> and a
<a href="https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf">model</a> from <a href="https://www.nvidia.com/en-us/deep-learning-ai/developer/?ncid=pa-pai-cs27dl-4165&gclid=CM37n4LQ0dECFUWVfgodzMsIZw">NVIDIA</a>.  I even considered adapting Yann LeCun's <a href="http://yann.lecun.com/exdb/mnist/">MNIST
model</a> for recognizing hand-written digits and even my own
TensorFlow model for Traffic-Sign classification.  In the end, I
chose the NVIDIA model, since it is relatively simple, it is
well-tested, and it reportedly produces good results.  
</p>

<p>
In general, I observe several salient facts about the NVIDIA
model.
</p>

<ul class="org-ul">
<li>It consists of 9 layers, including a normalization layer, 5
convolutional layers, and 3 fully-connected layers.
</li>
<li>The input image is split into YUV planes.
</li>
<li>The normalization layer is non-trainable.
</li>
<li>The 5 convolutional layers have 3 with a 2x2 stride and 5x5
kernel and 2 non-strided with a 3x3 kernel.
</li>
<li>The convolutional layers increase in depth of filters while
decreasing the image size.
</li>
<li>Following the convolutional layers is non-trainable flattening
layer.
</li>
<li>The flattening layer feeds into the sequence of 3
fully-connected layers.
</li>
<li>The fully-connected layers decrease in hidden-units.
</li>
<li>The last fully-connected layer feeds into a single non-trainable
node to produce real-valued output (steering angles).
</li>
<li>This being essentially a regression problem rather than a
classification problem, the model does not terminate with a
<a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> activation function and/or <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function and does not
employ <a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a>.
</li>
<li>While presumably they use activation functions within their
network, the NVIDIA researches do not seem to specify which
activation function(s) they use.
</li>
<li>Likewise, they seem not to use <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">pooling</a> layers or <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout">dropout</a>
      layers.
</li>
</ul>

<p>
In addition to their model architecture the NVIDIA researchers
also discuss augmentation and training.  
</p>

<ul class="org-ul">
<li>They augment their data by adding artificial shifts and
rotations.
</li>
<li>They transform their training data to account for the human
drivers' possible departure from what they call the "ground
truth", which is the road's centerline.
</li>
<li>They train their model by minimizing the mean squared error
("mse") between the steering command output by the network and
the command in the target data.  These correspond to our
steering angles.
</li>
</ul>

<p>
Finally, I observe some aspects of the comma.ai model, by way of
comparing and contrasting with the NVIDIA model.
</p>

<ul class="org-ul">
<li>The model uses the ELU <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> in its layers.
</li>
<li>The model normalizes the data like the NVIDIA model does, but it
is obvious that the data are normalized so that each of the RGB
pixel values in [0,255] is scaled to real-valued numbers in
[-1.0, 1.0].
</li>
<li>The comma.ai researchers use the <a href="https://keras.io/optimizers/#adam">ADAM</a> optimizer, a nice
general-purpose optimizer with good performance and which does
not require the specification of learning-rate parameters.
</li>
<li>They also optimize the mse.
</li>
</ul>

<p>
Synthesizing some of these concepts, I experimented with a family
of models that all had these general properties.
</p>

<ul class="org-ul">
<li>stacks of scaling, convolutional, flattening, fully-connected,
and readout layers
</li>
<li>start with a non-trainable normalization layer that scales the
input so that each pixel color channel is in [-1.0, 1.0].
</li>
<li>no sigmoid, softmax, or one-hot encoding
</li>
<li>use either ELU or RELU activation functions
</li>
<li>consider adding pooling and dropout layers
</li>
<li>optimize mse using the ADAM optimizer so that at least I do not
have to worry about learning rate parameters
</li>
</ul>

<p>
Of note, I also have these departures from the aforementioned
architectures.
</p>

<ul class="org-ul">
<li>experiment with different cropping sizes and strategies
</li>
<li>add a layer or layers (non-trainable) to do the cropping right
in the model
</li>
<li>experiment with adding a layer or layers (non-trainable) to
reduce the image size before feeding the data into the trainable
convolutional layers
</li>
<li>experiment with a non-trainable readout layer using the <a href="https://en.wikipedia.org/wiki/Activation_function">tanh</a>
activation function to constrain the output to [-1, 1],
appropriate for our steering angles
</li>
<li>little or no augmentation, though the cropping and especially
the image flipping (if used) might be considered augmentation of
a sort
</li>
<li>no manual transformation of the training data to a "ground
truth" like the NVIDIA researchers did
</li>
<li>the images are encoded in RGB planes rather than YUV planes
unless it becomes necessary to do so
</li>
<li>we add a non-trainable <a href="https://keras.io/layers/pooling/">AveragePooling2D</a> layer to act as an image
resizer. 
</li>
</ul>

<p>
With all of these considerations in mind, I conducted many
experiments with different combinations of factors, a saga that I
will not recount here.  In the end, the model that I settled on
had these features.
</p>

<ul class="org-ul">
<li>non-trainable cropping, resizing, and normalization layers
</li>
<li>alternating convolutional and max-pooling layers
</li>
<li>one dropout layer after the convolutional/max-pooling layers and
before the flattening layer
</li>
<li>fully-connected layers
</li>
<li>non-trainable readout layer with no activation
</li>
<li>RELU activations rather than ELU activations
</li>
</ul>

<p>
Finally, I should add that for image augmentation I briefly
considered performing random shifts of the input data, but in the
end decided that it was not necessary.
</p>
</div>

<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1">Model</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
The actual model is laid out in Keras code below.  It is coded
as a function that returns a Keras <code>model</code>.  Note that the
function does take the <code>input_shape</code> and the <code>crop_shape</code>.
Though I could perform image resizing (such as with
<code>cv2.resize</code>) and cropping outside of the model, I actually do
them inside the model.  This has several advantages.
</p>

<ol class="org-ol">
<li>It simplifies the code.
</li>
<li>Whatever cropping/resizing occurs <i>must</i> also be done in the
network service.  Performing these operations with the model
means that this is handled automatically, for free.
</li>
<li>In general, we might realize better training performance as
the cropping/resizing occur in the GPUs rather than in the
CPU.  In my particular case this did not occur because I
trained only on a laptop without a GPU.
</li>
</ol>

<p>
This means that the <code>input_shape</code> is not really a
hyper-parameter, since it is just the original image size, which
as we already have seen is (160, 320, 3).  The <code>crop_shape</code>
still is a hyper-parameter, of course.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">CarND</span>(input_shape, crop_shape):
    <span style="color: #fcaf3e;">model</span> = Sequential()

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Crop</span>
    <span style="color: #73d216;"># </span><span style="color: #73d216;">model.add(Cropping2D(((80,20),(1,1)), input_shape=input_shape, name="Crop"))</span>
    model.add(Cropping2D(crop_shape, input_shape=input_shape, name=<span style="color: #e9b96e;">"Crop"</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Resize</span>
    model.add(AveragePooling2D(pool_size=(1,4), name=<span style="color: #e9b96e;">"Resize"</span>, trainable=<span style="color: #e9b2e3;">False</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Normalize input.</span>
    model.add(Lambda(<span style="color: #b4fa70;">lambda</span> x: x/127.5 - 1., name=<span style="color: #e9b96e;">"Normalize"</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Reduce dimensions through trainable convolution, activation, and</span>
    <span style="color: #73d216;"># </span><span style="color: #73d216;">pooling layers.</span>
    model.add(Convolution2D(24, 3, 3, subsample=(2,2), name=<span style="color: #e9b96e;">"Convolution2D1"</span>, activation=<span style="color: #e9b96e;">"relu"</span>))
    model.add(MaxPooling2D(name=<span style="color: #e9b96e;">"MaxPool1"</span>))
    model.add(Convolution2D(36, 3, 3, subsample=(1,1), name=<span style="color: #e9b96e;">"Convolution2D2"</span>, activation=<span style="color: #e9b96e;">"relu"</span>))
    model.add(MaxPooling2D(name=<span style="color: #e9b96e;">"MaxPool2"</span>))
    model.add(Convolution2D(48, 3, 3, subsample=(1,1), name=<span style="color: #e9b96e;">"Convolution2D3"</span>, activation=<span style="color: #e9b96e;">"relu"</span>))
    model.add(MaxPooling2D(name=<span style="color: #e9b96e;">"MaxPool3"</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Dropout for regularization</span>
    model.add(Dropout(0.1, name=<span style="color: #e9b96e;">"Dropout"</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Flatten input in a non-trainable layer before feeding into</span>
    <span style="color: #73d216;"># </span><span style="color: #73d216;">fully-connected layers.</span>
    model.add(Flatten(name=<span style="color: #e9b96e;">"Flatten"</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Model steering through trainable layers comprising dense units</span>
    <span style="color: #73d216;"># </span><span style="color: #73d216;">as ell as dropout units for regularization.</span>
    model.add(Dense(100, activation=<span style="color: #e9b96e;">"relu"</span>, name=<span style="color: #e9b96e;">"FC2"</span>))
    model.add(Dense(50, activation=<span style="color: #e9b96e;">"relu"</span>, name=<span style="color: #e9b96e;">"FC3"</span>))
    model.add(Dense(10, activation=<span style="color: #e9b96e;">"relu"</span>, name=<span style="color: #e9b96e;">"FC4"</span>))

    <span style="color: #73d216;"># </span><span style="color: #73d216;">Generate output (steering angles) with a single non-trainable</span>
    <span style="color: #73d216;"># </span><span style="color: #73d216;">node.</span>
    model.add(Dense(1, name=<span style="color: #e9b96e;">"Readout"</span>, trainable=<span style="color: #e9b2e3;">False</span>))
    <span style="color: #b4fa70;">return</span> model
</pre>
</div>

<p>
Here is a summary of the actual model, as generated directly by
<code>model.summary</code> in Keras.
</p>

<pre class="example">
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
Crop (Cropping2D)                (None, 60, 318, 3)    0           cropping2d_input_14[0][0]        
____________________________________________________________________________________________________
Resize (AveragePooling2D)        (None, 60, 79, 3)     0           Crop[0][0]                       
____________________________________________________________________________________________________
Normalize (Lambda)               (None, 60, 79, 3)     0           Resize[0][0]                     
____________________________________________________________________________________________________
Convolution2D1 (Convolution2D)   (None, 29, 39, 24)    672         Normalize[0][0]                  
____________________________________________________________________________________________________
MaxPool1 (MaxPooling2D)          (None, 14, 19, 24)    0           Convolution2D1[0][0]             
____________________________________________________________________________________________________
Convolution2D2 (Convolution2D)   (None, 12, 17, 36)    7812        MaxPool1[0][0]                   
____________________________________________________________________________________________________
MaxPool2 (MaxPooling2D)          (None, 6, 8, 36)      0           Convolution2D2[0][0]             
____________________________________________________________________________________________________
Convolution2D3 (Convolution2D)   (None, 4, 6, 48)      15600       MaxPool2[0][0]                   
____________________________________________________________________________________________________
MaxPool3 (MaxPooling2D)          (None, 2, 3, 48)      0           Convolution2D3[0][0]             
____________________________________________________________________________________________________
Dropout (Dropout)                (None, 2, 3, 48)      0           MaxPool3[0][0]                   
____________________________________________________________________________________________________
Flatten (Flatten)                (None, 288)           0           Dropout[0][0]                    
____________________________________________________________________________________________________
FC2 (Dense)                      (None, 100)           28900       Flatten[0][0]                    
____________________________________________________________________________________________________
FC3 (Dense)                      (None, 50)            5050        FC2[0][0]                        
____________________________________________________________________________________________________
FC4 (Dense)                      (None, 10)            510         FC3[0][0]                        
____________________________________________________________________________________________________
Readout (Dense)                  (None, 1)             0           FC4[0][0]                        
====================================================================================================
Total params: 58,544
Trainable params: 58,544
Non-trainable params: 0
____________________________________________________________________________________________________
</pre>

<p>
And, here is a visualization of the model, as provided by the
<code>plot</code> function in Keras.
</p>

<div class="org-src-container">

<pre class="src src-python">plot(CarND([160, 320, 3], ((80,20),(1,1))), to_file=<span style="color: #e9b96e;">"model.png"</span>, show_shapes=<span style="color: #e9b2e3;">True</span>)
</pre>
</div>


<div class="figure">
<p><img src="model.png" alt="model.png" />
</p>
<p><span class="figure-number">Figure 18:</span> CarND Neural-Net Architecture</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Training</h3>
<div class="outline-text-3" id="text-2-3">
</div><div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1">Data Pipeline</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
The data-processing pipeline is rather simple, given the
composeable generator and non-generator utility functions
defined above.  The only real wrinkle is that we may want to do
random sample flipping (of both the image and its corresponding
steering angle) during training but we probably do not need to
do so for the validation data.  Since assembling the pipeline is
otherwise very similar for both the training and validation
data, and both should result in a generator that yields batches
consumable by the <a href="https://keras.io/models/sequential/"><code>model.fit_generator</code></a> function of Keras's
Sequential model, both pipelines are assembled in a function.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">pipeline</span>(theta, training=<span style="color: #e9b2e3;">False</span>):
    <span style="color: #73d216;"># </span><span style="color: #73d216;">randomly cycle through cached, loaded samples (images + angles)</span>
    <span style="color: #fcaf3e;">samples</span> = select(rcycle(fetch(select(split(feed(theta.trainingfile)), [0,3]), theta.base_path)), [0,1])
    <span style="color: #73d216;"># </span><span style="color: #73d216;">for training we might do sample flipping but no need for validation</span>
    <span style="color: #b4fa70;">if</span> training:
        <span style="color: #b4fa70;">if</span> theta.flip:
            <span style="color: #fcaf3e;">samples</span> = (sflip(x) <span style="color: #b4fa70;">for</span> x <span style="color: #b4fa70;">in</span> samples)
    <span style="color: #73d216;"># </span><span style="color: #73d216;">group the samples</span>
    <span style="color: #fcaf3e;">groups</span> = group(samples, theta.batch_size)
    <span style="color: #73d216;"># </span><span style="color: #73d216;">turn the groups into batches (NumPy arrays)</span>
    <span style="color: #fcaf3e;">batches</span> = batch(transpose(groups))
    <span style="color: #73d216;"># </span><span style="color: #73d216;">return the batch generator</span>
    <span style="color: #b4fa70;">return</span> batches
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2">Training</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
With functions for constructing the model architecture and the
data pipelines in hand, training is very simple.  One additional
item to point out is this.  There can be a proliferation of
<i>literal</i> hyper-parameters (crop sizes, epochs, batch sizes,
whether or not to do random flipping, etc.) and passing these
parameters among all the related and nested functions can be a
nuisance.  I find it convenient to collect such parameters into
a handy data structure that can be instantiated as a global
variable, and then all of the functions can access the
parameters that they need.  In Python, two obvious candidates
that leap to mind are <a href="https://docs.python.org/3/tutorial/datastructures.html">dictionaries</a> and <a href="https://docs.python.org/3/tutorial/classes.html">classes</a>.  Python
dictionaries are slightly easier to create, but slightly more of
a nuisance to use, whereas the reverse is true for Python
classes/objects.  I elected to use an global instance <code>theta</code> of
a class <code>HyperParameters</code>, but this is not considered to be a
very important point.
</p>

<p>
Another point worth discussing relates to "cropping."  As
discussed above in the data analysis and architecture sections
and below in the summary, cropping of the data is an option that
I took.  The <a href="#coderef-crop_shape"class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-crop_shape');" onmouseout="CodeHighlightOff(this, 'coderef-crop_shape');"><code>theta.crop_shape</code></a> hyper-parameter that appears
below sets the cropping window, and it works as follows.
</p>

<p>
The <code>theta.crop_shape</code> parameter is dictated by the <a href="https://keras.io/layers/convolutional/#cropping2d">Cropping2D</a>
layer to be a tuple of tuples.  The first tuple sets the number
of pixels to be cropped from the top and bottom edges along the
image height direction, and the second tuple sets the number of
pixels to be cropped from the left and right edges of the width
direction.  As you can see, the top and bottom are cropped with
<code>(80, 20)</code> which as in the figure above, removes much of the
image above the horizon and some of the image where the car hood
is superimposed.  Those values were chosen by experimentation,
and the choices are discussed below.  However, the <code>(1,1)</code> value
for the width cropping merits some explanation.  In principle, I
saw no reason to crop anything in the width direction.  However,
Keras has a bug (fixed but not yet in a stable release) such
that Cropping2D fails when any of the four elements in the tuple
of tuples is 0.  This is the reason for the 1 pixel crops on the
left and right edges.
</p>

<p>
Now, as a sanity check, we conduct a small training (3 epochs,
30 samples per epoch, batch size 10) of the data in
<code>driving_log_overtrain.csv</code>.  This is just to "get our feet wet"
and quickly to verify that the code written above even works.
Note that we use the same file for the validation set.  This is
just a test, so it does not really matter what we use for the
validation set.
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #b4fa70;">class</span> <span style="color: #8cc4ff;">HyperParameters</span>:
    <span style="color: #b4fa70;">def</span> <span style="color: #fce94f;">__init__</span>(<span style="color: #b4fa70;">self</span>):
        <span style="color: #b4fa70;">return</span>

<span style="color: #fcaf3e;">theta</span> = HyperParameters()
<span style="color: #fcaf3e;">theta.input_shape</span> = [160, 320, 3]
<span id="coderef-crop_shape" class="coderef-off"><span style="color: #fcaf3e;">theta.crop_shape</span> = ((80,20),(1,1))   <span style="color: #73d216;"># </span><span style="color: #73d216;">crop size</span></span>
<span style="color: #fcaf3e;">theta.samples_per_epoch</span> = 30
<span style="color: #fcaf3e;">theta.valid_samples_per_epoch</span> = 30
<span style="color: #fcaf3e;">theta.epochs</span> = 3
<span style="color: #fcaf3e;">theta.batch_size</span> = 10
<span style="color: #fcaf3e;">theta.trainingfile</span> = <span style="color: #e9b96e;">"data/driving_log_overtrain.csv"</span>
<span style="color: #fcaf3e;">theta.validationfile</span> = <span style="color: #e9b96e;">"data/driving_log_overtrain.csv"</span>
<span style="color: #fcaf3e;">theta.base_path</span> = <span style="color: #e9b96e;">"data/"</span>
<span style="color: #fcaf3e;">theta.flip</span> = <span style="color: #e9b2e3;">False</span>

<span style="color: #fcaf3e;">model</span> = CarND(theta.input_shape, theta.crop_shape)
model.<span style="color: #e090d7;">compile</span>(loss=<span style="color: #e9b96e;">"mse"</span>, optimizer=<span style="color: #e9b96e;">"adam"</span>)

<span style="color: #fcaf3e;">traingen</span> = pipeline(theta, training=<span style="color: #e9b2e3;">True</span>)
<span style="color: #fcaf3e;">validgen</span> = pipeline(theta)

<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
<span style="color: #fcaf3e;">history</span> = model.fit_generator(
    traingen,
    theta.samples_per_epoch,
    theta.epochs,
    validation_data=validgen,
    verbose=2,
    nb_val_samples=theta.valid_samples_per_epoch)
</pre>
</div>

<p>
Next, we perform the actual training on the
<code>driving_log_train.csv</code> file, validating against the
<code>driving_log_validation.csv</code> file.  After <i>this</i> training we
actually save the model to <code>model.json</code> and the model weights to
<code>model.h5</code>, files suitable for input into the network service in
<code>drive.py</code>.  
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fcaf3e;">theta</span> = HyperParameters()
<span style="color: #fcaf3e;">theta.input_shape</span> = [160, 320, 3]
<span style="color: #fcaf3e;">theta.crop_shape</span> = ((80,20),(1,1))
<span style="color: #fcaf3e;">theta.trainingfile</span> = <span style="color: #e9b96e;">"data/driving_log_train.csv"</span>
<span style="color: #fcaf3e;">theta.validationfile</span> = <span style="color: #e9b96e;">"data/driving_log_validation.csv"</span>
<span style="color: #fcaf3e;">theta.base_path</span> = <span style="color: #e9b96e;">"data/"</span>
<span style="color: #fcaf3e;">theta.samples_per_epoch</span> = 7036
<span style="color: #fcaf3e;">theta.valid_samples_per_epoch</span> = 1000
<span style="color: #fcaf3e;">theta.epochs</span> = 3
<span style="color: #fcaf3e;">theta.batch_size</span> = 100
<span style="color: #fcaf3e;">theta.flip</span> = <span style="color: #e9b2e3;">False</span>

<span style="color: #fcaf3e;">model</span> = CarND(theta.input_shape, theta.crop_shape)
model.<span style="color: #e090d7;">compile</span>(loss=<span style="color: #e9b96e;">"mse"</span>, optimizer=<span style="color: #e9b96e;">"adam"</span>)

<span style="color: #fcaf3e;">traingen</span> = pipeline(theta, training=<span style="color: #e9b2e3;">True</span>)
<span style="color: #fcaf3e;">validgen</span> = pipeline(theta)

<span style="color: #b4fa70;">print</span>(<span style="color: #e9b96e;">""</span>)
<span style="color: #fcaf3e;">history</span> = model.fit_generator(
    traingen,
    theta.samples_per_epoch,
    theta.epochs,
    validation_data=validgen,
    verbose=2,
    nb_val_samples=theta.valid_samples_per_epoch)
model.save_weights(<span style="color: #e9b96e;">"model.h5"</span>)
<span style="color: #b4fa70;">with</span> <span style="color: #e090d7;">open</span>(<span style="color: #e9b96e;">"model.json"</span>, <span style="color: #e9b96e;">"w"</span>) <span style="color: #b4fa70;">as</span> f:
    f.write(model.to_json())
</pre>
</div>

<pre class="example">
&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;
... ... ... ... ... ... Epoch 1/3
139s - loss: 0.0132 - val_loss: 0.0126
Epoch 2/3
31s - loss: 0.0106 - val_loss: 0.0083
Epoch 3/3
27s - loss: 0.0099 - val_loss: 0.0092
... ... 4042
</pre>

<p>
Ta-da!  We now have a trained model in <code>model.json</code> and
<code>model.h5</code> that we can run in the simulator with this command.
</p>

<div class="org-src-container">

<pre class="src src-python">python drive.py model.json
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Summary</h2>
<div class="outline-text-2" id="text-3">
<p>
This was an enjoyable and illuminating, albeit challenging project.
Ultimately, we believe we achieved the primary objectives of the
project.
</p>

<ul class="org-ul">
<li>The code is functional and successfully operates the simulator (on
Track 1, at least).
</li>
<li>The code is factored, organized, commented, and documented.
</li>
<li>The code uses Python generators and the <code>model.fit_generator</code>
    function in Keras.
</li>
<li>The training data are normalized.
</li>
<li>The code uses a neural network that has convolutional layers.
</li>
<li>There are activation layers (RELU) to introduce non-linearity into
the model.
</li>
<li>The data have been split into training and validation sets.  Note
that no test data split was applied, since testing ultimately is
done by successfully operating the car in the simulator.
</li>
<li>The model has a dropout layer to help reduce overfitting.
</li>
<li>No <code>tanh</code> activation function was needed in the final readout
layer.
</li>
<li>Training is done with the ADAM optimizer so that learning-rate
parameters need not be fiddled with.
</li>
<li>While we were prepared to record training data ourselves if
necessary, in the end we succeeded using only the Udacity data.
</li>
</ul>

<p>
Finally, we make the following observations.
</p>

<ul class="org-ul">
<li>After much experimentation, it turns out that cropping the image
was the single most important factor in achieving good results.
</li>
<li>From this observation we make the following inferences.
</li>
<li>The "sky" portion of the image is largely irrelevant for
generating appropriate steering angles.
</li>
<li>In principle, the neural network could have learned that on its
own, but helping it out by manually cropping out the sky was
enormously helpful.
</li>
<li>In fact, we actually cropped out more than just the sky, removing
some of the road near the horizon.
</li>
<li>When the road is curved, the portion of the road nearer to the
horizon naturally has the greatest apparent curvature when
projected onto the 2-D image plane.  Conversely, the curvature of
the road near the car is barely apparent.
</li>
<li>The portion of the road nearer to the horizon essentially provides
information <i>about the future</i>.  That is, it establishes road
conditions that the car <i>will</i> encounter.
</li>
<li>The portion of the road nearer to the bottom essentially provides
information <i>about the present</i>.  That is, it registers the
position of the car left to right, with respect to the edges and
centerline of the road.
</li>
<li>This is a very simple neural network model, and we have
made no effort whatsoever to introduce anything like "memory" or
"anticipatory" behavior.
</li>
<li>Our primary goal is <i>safely</i> to navigate the road by staying away
from its edges.
</li>
<li>Navigation (e.g., route planning) and choosing the best line
(e.g., cutting the corners for racing purposes) are non-goals.
</li>
<li>Consequently, it may very well be that the bottom 1/3 to 1/4 of
the road really has the most relevance for the task at hand.
</li>
<li>Cropping the image, and also resizing the image helped
dramatically reduce the size of the model in terms of the number
of parameters, which greatly helped training time.
</li>
<li>Cropping and resizing right in the model, rather than as a
pre-processing step, simplified the code and adapted it perfectly
for the network service in <code>drive.py</code>.
</li>
<li>Cropping the images ironically may have ameliorated the left-turn
bias in the training data.
</li>
<li>In earlier experiments, before cropping, there always was a
pronounced leftward drift of the car unless image reflection or
flipping was applied.
</li>
<li>In those early experiments, image flipping thoroughly removed the
leftward drift.
</li>
<li>However, the car still had significant problems on sharp turns and
<i>especially</i> with the "pit stop" after the bridge.
</li>
<li>After adding the cropping of <code>(80,120)</code> for the top and bottom,
the car was able to negotiate the sharp turns and avoid the pit
stop.
</li>
<li>Moreover, after adding the cropping the leftward drift largely
disappeared <i>even without image flipping</i>.
</li>
<li>We speculate that the influence of the left-turn bias is greatest
within the upper portions of the image, but the model has no way
of knowing that.
</li>
<li>By cropping out much of the upper portions of the image we seem to
have removed that bias.
</li>
<li>Incidentally, the number of epochs was chosen largely by
trial-and-error.  We ran experiments training with a large number
of epochs, hoping to see the accuracy on the validation set turn
around and start to diverge.  Unfortunately, we never saw that and
the validation error continued to decline.
</li>
<li>In principle, that should be good.  However, in practice, we
noticed that models trained for many epochs, with very low
validation accuracy, drove somewhat poorly.
</li>
<li>We may not actually have needed the droput layer.  In fact, in
some experiments we seem to have converged to a better-performing
model more quickly <i>without</i> dropout.
</li>
<li>It may be that without dropout, the model overtrains on Track 1,
but that since we are validating on Track 1 it does not produce a
problem.  If true then it is likely such an overtrained model
without dropout would perform <i>more</i> poorly on other tracks.
</li>
<li>For that reason, we kept the dropout layer.  We plan to perform
more experiments and strive to achieve satisfactory performance on
Track 2 after <i>only</i> training on Udacity's data for Track 1.
</li>
<li>As discussed earlier, I had considered masking out small-angle
samples.  However, I achieved satisfactory results without doing
this.  It occurs to us that in fact these may be crucial data for
stabilizing the car within the center of the track.

<p>
One final thought is this.  In the end, the particular network
architecture that I chose may be overkill.  It may be that a much
simpler model would suffice.  As described earlier, I began with
the NVIDIA model, which while simple still may be more than is
needed here.  After all, that model drove a real car in real-world
conditions, trained with real-world data.  Our problem confronts
only a simulator.  Further work may involve drastically reducing
the size and complexity of the model to see where it finally
breaks down.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Final Model Characteristics</caption>

<colgroup>
<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Hyper-Parameter</th>
<th scope="col" class="left">Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">Data</td>
<td class="left">Udacity data</td>
</tr>

<tr>
<td class="left">Epochs</td>
<td class="left">3</td>
</tr>

<tr>
<td class="left">Samples per epoch</td>
<td class="left">7036</td>
</tr>

<tr>
<td class="left">Validation samples per epoch</td>
<td class="left">1000</td>
</tr>

<tr>
<td class="left">Batch size</td>
<td class="left">100</td>
</tr>

<tr>
<td class="left">Random sample flipping</td>
<td class="left">no</td>
</tr>

<tr>
<td class="left">Top crop</td>
<td class="left">80 pixels</td>
</tr>

<tr>
<td class="left">Bottom crop</td>
<td class="left">20 pixels</td>
</tr>

<tr>
<td class="left">RGB or YUV image encoding</td>
<td class="left">RGB</td>
</tr>

<tr>
<td class="left">Training time</td>
<td class="left">under 2 minutes</td>
</tr>

<tr>
<td class="left">Camera</td>
<td class="left">center-only</td>
</tr>

<tr>
<td class="left">Augmentation</td>
<td class="left">none</td>
</tr>
</tbody>
</table>
</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: David A. Ventimiglia (<a href="mailto:dventimi@gmail.com">dventimi@gmail.com</a>)</p>
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2017-01-19&gt;</span></span></p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
